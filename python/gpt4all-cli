#!/usr/bin/env python

import io
import re
from os import environ
from os.path import join, isfile, realpath, expanduser
from gnureadline import parse_and_bind, set_completer, read_history_file, write_history_file
from argparse import ArgumentParser
from contextlib import contextmanager
from signal import signal, SIGINT, SIGALRM, setitimer, ITIMER_REAL
from textwrap import dedent
from glob import glob

from lark import Lark
from lark.visitors import Interpreter
from gpt4all import GPT4All

from gpt4all_cli import *

REVISION:str|None
try:
  REVISION=environ["GPT4ALLCLI_REVISION"]
except Exception:
  try:
    from subprocess import check_output, DEVNULL
    REVISION=check_output(['git', 'rev-parse', 'HEAD'],
                          cwd=environ['GPT4ALLCLI_ROOT'],
                          stderr=DEVNULL).decode().strip()
  except Exception:
    try:
      from gpt4all_cli.revision import REVISION as __rv__
      REVISION = __rv__
    except ImportError:
      REVISION = None

def _get_cmd_prefix():
  prefices = list({c[0] for c in COMMANDS if len(c)>0})
  assert len(prefices) == 1
  return prefices[0]

CMDPREFIX = _get_cmd_prefix()

def read_multiline_input(initial_prompt=">>> ", intermediate_prompt="... "):
  lines = []
  prompt = initial_prompt
  while True:
    try:
      line = input(prompt)
      lines.append(line)
      if line=='' or (len(line)>0 and line[0] in CMDPREFIX):
        break
      prompt = intermediate_prompt
    except EOFError:
      break
  return lines

ARG_PARSER = ArgumentParser(description="Command-line arguments")
ARG_PARSER.add_argument(
  "--model-dir",
  type=str,
  help="Model directory to prepend to model file names",
  default=None
)
ARG_PARSER.add_argument(
  "--model", "-m",
  type=str,
  help="Model to use for chatbot",
  # default="mistral-7b-instruct-v0.1.Q4_0.gguf",
  # default='/home/grwlf/.local/share/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf'
  default=None
)
ARG_PARSER.add_argument(
  "--num-threads", "-t",
  type=int,
  help="Number of threads to use for chatbot",
  default=None
)
ARG_PARSER.add_argument(
  "--model-temperature",
  type=float,
  help="Temperature parameter of the model",
  default=None
)
ARG_PARSER.add_argument(
  "--device", "-d",
  type=str,
  help="Device to use for chatbot, e.g. gpu, amd, nvidia, intel. Defaults to CPU.",
  default=None
)
ARG_PARSER.add_argument(
  "--readline-key-send",
  type=str,
  help="Terminal code to treat as Ctrl+Enter (default: \\C-k)",
  default="\\C-k"
)
ARG_PARSER.add_argument(
  '--readline-prompt',
  type=str,
  help="Input prompt (default: >>>)",
  default=">>> "
)
HISTORY_DEF="_gpt4all_cli_history"
ARG_PARSER.add_argument(
  '--readline-history',
  type=str,
  metavar='FILE',
  help=f"History file name (default is '{HISTORY_DEF}'; set empty to disable)",
  default=HISTORY_DEF
)
ARG_PARSER.add_argument(
  '--revision',
  action='store_true',
  help="Print the revision",
)



@contextmanager
def with_sigint(_handler):
  """ A Not very correct singal handler. One also needs to mask signals during switching handlers """
  prev=signal(SIGINT,_handler)
  try:
    yield
  finally:
    signal(SIGINT,prev)

def as_float(val:str, default:float|None)->float|None:
  return float(val) if val not in {None,"default"} else default
def as_int(val:str, default:int|None)->int|None:
  return int(val) if val not in {None,"default"} else default

def ask1(args, gpt4all, message:str) -> str|None:
  response = io.StringIO()
  break_request = False
  try:
    def _signal_handler(signum,frame):
      nonlocal break_request
      print("\n<Keyboard interrupt>")
      break_request = True

    def _model_callback(*args, **kwargs):
      return not break_request

    response_generator = gpt4all.generate(
      message,
      # preferential kwargs for chat ux
      max_tokens=200,
      temp=as_float(args.model_temperature,0.9),
      top_k=40,
      top_p=0.9,
      min_p=0.0,
      repeat_penalty=1.1,
      repeat_last_n=64,
      n_batch=9,
      # required kwargs for cli ux (incremental response)
      streaming=True,
      callback=_model_callback
    )

    with with_sigint(_signal_handler):
      for token in response_generator:
        print(token, end='', flush=True)
        response.write(token)

  finally:
    response.close()
    print()
  return response

PARSER = Lark(GRAMMAR, start='start')


def test_parser():
  def _assert(a, tb):
    ta = re.sub(r"^[ \t]+$", '', PARSER.parse(a).pretty().strip().replace('\t', ' '*7), flags=re.MULTILINE)
    tb = re.sub(r"^[ \t]+$", '', dedent(tb).strip().replace('\t', ' '*7), flags=re.MULTILINE)
    assert ta == tb, f"\nExpected:\n{tb}\nGot:\n{ta}"

  _assert(r'\a', r'''
    start
      escape       \a
  ''')
  _assert('/echo ', '''
    start
      command       /echo
      text
  ''')
  _assert('/echo', '''
    start
      command       /echo
  ''')
  _assert('a/echo', '''
    start
      text       a
      command       /echo
  ''')
  _assert('/echoa', '''
    start
      command       /echo
      text       a
  ''')
  _assert(r'\/echo', r'''
    start
      escape       \/
      text       echo
  ''')
  _assert(r'/echo/echo', r'''
    start
      command       /echo
      command       /echo
  ''')
  _assert('/echo/echoxx', r'''
    start
      command       /echo
      command       /echo
      text       xx
  ''')
  _assert('/model "aaa"', r'''
    start
      command
        /model

        string       "aaa"
  ''')
  _assert(r'/echo\a', r'''
    start
      command       /echo
      escape       \a
  ''')
  _assert(r'', r'''
    start
  ''')
  _assert(r'/nthreads 3', r'''
    start
      command
        /nthreads

        number       3
  ''')
  _assert('/temp 3.4', r'''
    start
      command
        /temp

        float       3.4
  ''')
  _assert('/temp default', r'''
    start
      command
        /temp

        def
  ''')

def print_help():
  print(f"Commands: {' '.join(COMMANDS)}")

def print_aux(args, s:str)->None:
  print(f"INFO: {s}")

def print_aux_err(args, s:str)->None:
  print(f"ERROR: {s}")

@contextmanager
def chat_session(gpt4all):
  if gpt4all is None:
    yield
  else:
    with gpt4all.chat_session():
      yield

def model_locations(args, model)->list[str]:
  for path in [model] + ([join(args.model_dir,model)] if args.model_dir else []):
    for match in glob(expanduser(path)):
      yield realpath(match)

def main(cmdline=None):
  args = ARG_PARSER.parse_args(cmdline)
  if args.revision:
    print(REVISION)
    return 0
  test_parser()
  parse_and_bind('tab: complete')
  parse_and_bind(f'"{args.readline_key_send}": "{CMD_ASK}\n"')
  hint = args.readline_key_send.replace('\\', '')

  print(f"Type /help or a question followed by the /ask command (or by pressing "
        f"`{hint}` key).")
  old_model = None
  gpt4all = None

  def _apply():
    nonlocal gpt4all, old_model
    if old_model != args.model:
      if args.model is not None:
        gpt4all = GPT4All(args.model, device=args.device)
        print_aux(args, "Model is now initialized")
      else:
        if gpt4all:
          print_aux(args, "Model is now destroyed")
          del gpt4all
        gpt4all = None
      old_model = args.model
    if gpt4all is not None:
      num_threads = as_int(args.num_threads, None)
      old_num_threads = gpt4all.model.thread_count()
      if num_threads is not None:
        gpt4all.model.set_thread_count(num_threads)
        new_num_threads = gpt4all.model.thread_count()
        if old_num_threads != new_num_threads:
          print_aux(args, f"Num threads is now {new_num_threads}")

  class Repl(Interpreter):
    def __init__(self):
      self._reset()
    def _reset(self):
      self.in_echo = False
      self.message = ""
      self.exit_request = False
      self.reset_request = False
    def reset(self):
      old_message = self.message
      self._reset()
      if len(old_message)>0:
        print_aux(args, "Message buffer is now empty")
    def _finish_echo(self):
      if self.in_echo:
        print()
      self.in_echo = False
    def command(self, tree):
      self._finish_echo()
      command = tree.children[0].value
      if command == CMD_ECHO:
        self.in_echo = True
      elif command == CMD_ASK:
        if gpt4all is None:
          raise RuntimeError("No model is active, use /model first")
        if len(self.message.strip()) == 0:
          raise RuntimeError("Empty message buffer, write something first")
        else:
          ask1(args, gpt4all, self.message)
          self.message = ""
      elif command == CMD_HELP:
        print_help()
      elif command == CMD_EXIT:
        self.exit_request = True
      elif command == CMD_MODEL:
        matched = False
        argument = tree.children[2].children[0].value[1:-1]
        for model in model_locations(args, argument):
          if isfile(model):
            args.model = model
            self.reset_request = True
            matched = True
            break
        if not matched:
          args.model = argument if len(argument)>0 else None
        print_aux(args, f"Model will be set to '{args.model}'")
        self.reset_request = True
      elif command == CMD_NTHREADS:
        args.num_threads = tree.children[2].children[0].value
        print_aux(args, f"Num threads will be set to '{args.num_threads}'")
        self.reset_request = True
      elif command == CMD_TEMP:
        args.model_temperature = tree.children[2].children[0].value
        print_aux(args, f"Temperature will be set to '{args.model_temperature}'")
      elif command == CMD_RESET:
        print_aux(args, "Message buffer will be cleared")
        self.reset_request = True
      else:
        raise ValueError(f"Unknown command: {command}")
    def text(self, tree):
      text = tree.children[0].value
      if self.in_echo:
        print(text, end='')
      else:
        for cmd in COMMANDS:
          if cmd in text:
            print_aux(args, f"Warning: '{cmd}' was parsed as a text")
        self.message += text
    def escape(self, tree):
      text = tree.children[0].value[1:]
      if self.in_echo:
        print(text, end='')
      else:
        self.message += text

  if args.readline_history:
    try:
      read_history_file(args.readline_history)
      print_aux(args, f"History file loaded")
    except FileNotFoundError:
      print_aux(args, f"History file not loaded")
  else:
    print_aux(args, f"History file is not used")

  repl = Repl()
  while not repl.exit_request:
    _apply()
    repl.reset()
    with chat_session(gpt4all):
      try:
        while all([not repl.exit_request, not repl.reset_request]):
          repl.visit(PARSER.parse(input(args.readline_prompt)))
          repl._finish_echo()
          if args.readline_history:
            write_history_file(args.readline_history)
      except (ValueError,RuntimeError) as err:
        print_aux_err(args, err)
      except EOFError:
        print()
        break

if __name__ == "__main__":
  main()
